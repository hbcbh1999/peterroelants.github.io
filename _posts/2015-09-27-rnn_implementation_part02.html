---
layout: notebook_simple_rnn_post
title: How to implement a recurrent neural network Part 2
---

<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Binary-addition-with-a-non-linear-RNN">
    Binary addition with a non-linear RNN
    <a class="anchor-link" href="#Binary-addition-with-a-non-linear-RNN">
     ¶
    </a>
   </h2>
   <p>
    This part will show how to train a more complex RNN with input data stored as a
    <a href="#Dataset">
     tensor
    </a>
    and trained with
    <a href="#Rmsprop-with-momentum-optimisation">
     Rmsprop and Nesterov momentum
    </a>
    .
   </p>
   <p>
    While the
    <a href="{% post_url 2015-09-27-rnn_implementation_part01 %}">
     first part
    </a>
    of this tutorial described a simple linear recurrent network, this tutorial will describe an RNN with non-linear transfer functions that is able to learn how to perform
    <a href="https://en.wikipedia.org/wiki/Binary_number#Addition">
     binary addition
    </a>
    from examples.
   </p>
   <p>
    First we import the libraries we need and define the dataset.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [1]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython3">
     <pre><span></span><span class="c1"># Python imports</span>
<span class="c1"># Allow matplotlib to plot inside this notebook</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>  <span class="c1"># Matrix and vector computation package</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>  <span class="c1"># Plotting library</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>  <span class="c1"># Fancier plots</span>

<span class="c1"># Set seaborn plotting style</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">'darkgrid'</span><span class="p">)</span>
<span class="c1"># Set the seed for reproducability</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Print versions used</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Python: </span><span class="si">{}</span><span class="s1">.</span><span class="si">{}</span><span class="s1">.</span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">*</span><span class="n">sys</span><span class="o">.</span><span class="n">version_info</span><span class="p">[:</span><span class="mi">3</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'numpy: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'matplotlib: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'seaborn: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>Python: 3.6.6
numpy: 1.14.5
matplotlib: 2.2.3
seaborn: 0.9.0
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Dataset">
    Dataset
    <a class="anchor-link" href="#Dataset">
     ¶
    </a>
   </h2>
   <p>
    This tutorial uses a dataset of 2000 training samples to train the RNN that can be created with the
    <code>
     create_dataset
    </code>
    method defined below. Each sample consists of two 6-bit input numbers $(x_{i1}$, $x_{i2})$ padded with a 0 to make it 7 characters long, and a 7-bit target number ($t_{i}$) so that $t_{i} = x_{i1} + x_{i2}$ ($i$ is the sample index). The numbers are represented as
    <a href="https://en.wikipedia.org/wiki/Binary_number">
     binary numbers
    </a>
    with the
    <a href="https://en.wikipedia.org/wiki/Most_significant_bit">
     most significant bit
    </a>
    on the right (least significant bit first). This is so that our RNN can perform the addition form left to right.
   </p>
   <p>
    The input and target vectors are stored in a 3th-order tensor. A
    <a href="https://en.wikipedia.org/wiki/Tensor">
     tensor
    </a>
    is a generalisation of vectors and matrices, a vector is a 1st-order tensor, a matrix is a 2nd-order tensor. The order of a tensor is the dimensionality of the array data structure needed to represent it.
    <br/>
    The dimensions of our training data (
    <code>
     X_train
    </code>
    ,
    <code>
     T_train
    </code>
    ) are printed after the creation of the dataset below. The first order of our data tensors goes over all the samples (2000 samples), the second order goes over the variables per unit of time (7 timesteps), and the third order goes over the variables for each timestep and sample (e.g. input variables $x_{ik1}$, $x_{ik2}$ with $i$ the sample index and $k$ the timestep). The input tensor
    <code>
     X_train
    </code>
    is visualised in the following figure:
   </p>
   <p>
    <img alt="Visualisation of input tensor X" src="/images/RNN_implementation/SimpleRNN02_Tensor.png"/>
   </p>
   <p>
    The following code block initialises the dataset.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [2]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython3">
     <pre><span></span><span class="c1"># Create dataset</span>
<span class="n">nb_train</span> <span class="o">=</span> <span class="mi">2000</span>  <span class="c1"># Number of training samples</span>
<span class="c1"># Addition of 2 n-bit numbers can result in a n+1 bit number</span>
<span class="n">sequence_len</span> <span class="o">=</span> <span class="mi">7</span>  <span class="c1"># Length of the binary sequence</span>

<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">nb_samples</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">):</span>
    <span class="sd">"""Create a dataset for binary addition and </span>
<span class="sd">    return as input, targets."""</span>
    <span class="n">max_int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="n">sequence_len</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Maximum integer that can be added</span>
     <span class="c1"># Transform integer in binary format</span>
    <span class="n">format_str</span> <span class="o">=</span> <span class="s1">'{:0'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sequence_len</span><span class="p">)</span> <span class="o">+</span> <span class="s1">'b}'</span>
    <span class="n">nb_inputs</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># Add 2 binary numbers</span>
    <span class="n">nb_outputs</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Result is 1 binary number</span>
    <span class="c1"># Input samples</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_samples</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">,</span> <span class="n">nb_inputs</span><span class="p">))</span>
    <span class="c1"># Target samples</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nb_samples</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">,</span> <span class="n">nb_outputs</span><span class="p">))</span>
    <span class="c1"># Fill up the input and target matrix</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_samples</span><span class="p">):</span>
        <span class="c1"># Generate random numbers to add</span>
        <span class="n">nb1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_int</span><span class="p">)</span>
        <span class="n">nb2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_int</span><span class="p">)</span>
        <span class="c1"># Fill current input and target row.</span>
        <span class="c1"># Note that binary numbers are added from right to left, </span>
        <span class="c1">#  but our RNN reads from left to right, so reverse the sequence.</span>
        <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
            <span class="nb">reversed</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">format_str</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nb1</span><span class="p">)]))</span>
        <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
            <span class="nb">reversed</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">format_str</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nb2</span><span class="p">)]))</span>
        <span class="n">T</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
            <span class="nb">reversed</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">format_str</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nb1</span><span class="o">+</span><span class="n">nb2</span><span class="p">)]))</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span>

<span class="c1"># Create training samples</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">T_train</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">nb_train</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'X_train tensor shape: </span><span class="si">{X_train.shape}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'T_train tensor shape: </span><span class="si">{T_train.shape}</span><span class="s1">'</span><span class="p">)</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>X_train tensor shape: (2000, 7, 2)
T_train tensor shape: (2000, 7, 1)
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="Binary-addition">
    Binary addition
    <a class="anchor-link" href="#Binary-addition">
     ¶
    </a>
   </h3>
   <p>
    Performing binary addition is a good
    <a href="https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf">
     toy problem
    </a>
    to illustrate how recurrent neural networks process input streams into output streams. The network needs to learn how to carry a bit to the next state (memory) and when to output a 0 or 1 dependent on the input and state.
   </p>
   <p>
    The following code prints a visualisation of the inputs and target output we want our network to produce.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [3]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython3">
     <pre><span></span><span class="c1"># Show an example input and target</span>
<span class="k">def</span> <span class="nf">printSample</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">"""Print a sample in a more visual way."""</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">d</span><span class="p">))</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">x1</span><span class="p">])</span>
    <span class="n">x1_r</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">x1</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">d</span><span class="p">))</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">x2</span><span class="p">])</span>
    <span class="n">x2_r</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">x2</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">t</span><span class="p">])</span>
    <span class="n">t_r</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">t</span><span class="p">)),</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">y</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'x1:   </span><span class="si">{x1:s}</span><span class="s1">   </span><span class="si">{x1_r:2d}</span><span class="s1">'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'x2: + </span><span class="si">{x2:s}</span><span class="s1">   </span><span class="si">{x2_r:2d}</span><span class="s1">'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'      -------   --'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'t:  = </span><span class="si">{t:s}</span><span class="s1">   </span><span class="si">{t_r:2d}</span><span class="s1">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'y:  = </span><span class="si">{y:s}</span><span class="s1">'</span><span class="p">)</span>
    
<span class="c1"># Print the first sample</span>
<span class="n">printSample</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,:])</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>x1:   1010010   37
x2: + 1101010   43
      -------   --
t:  = 0000101   80
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Recurrent-neural-network-architecture">
    Recurrent neural network architecture
    <a class="anchor-link" href="#Recurrent-neural-network-architecture">
     ¶
    </a>
   </h2>
   <p>
    Our recurrent network will take 2 input variables for each sample for each timepoint, transform them to states, and output a single probability that the current output is $1$ (instead of $0$). The input is transformed into the states of the RNN where it can hold information so the network knows what to output the next timestep.
   </p>
   <p>
    There are many ways to visualise the RNN we are going to build. We can visualise the network as in the
    <a href="{% post_url 2015-09-27-rnn_implementation_part01 %}">
     previous part
    </a>
    of our tutorial and unfold the processing of each input, state-update and output of a single timestep separately from the other timesteps.
   </p>
   <p>
    <img alt="Structure of the RNN" src="/images/RNN_implementation/SimpleRNN02_1.png"/>
   </p>
   <p>
    Or we can view the processing of the full input, state-updates, and full output seperately from each other. The full input tensor can be
    <a href="https://en.wikipedia.org/wiki/Map_%28higher-order_function%29">
     mapped
    </a>
    in parallel to be used directly in the RNN state updates. And also the RNN states can be mapped in parallel to the output of each timestep.
   </p>
   <p>
    <img alt="Structure of the RNN tensor processing" src="/images/RNN_implementation/SimpleRNN02_2.png"/>
   </p>
   <p>
    The steps are abstracted in different classes below. Each class has a
    <code>
     forward
    </code>
    method that performs the
    <a href="{% post_url 2015-06-10-neural_network_implementation_part03 %}#1.-Forward-step">
     forward steps
    </a>
    of backpropagation, and a
    <code>
     backward
    </code>
    method that perform the
    <a href="{% post_url 2015-06-10-neural_network_implementation_part03 %}#2.-Backward-step">
     backward
    </a>
    steps of backpropagation.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="Processing-of-input-and-output-tensors">
    Processing of input and output tensors
    <a class="anchor-link" href="#Processing-of-input-and-output-tensors">
     ¶
    </a>
   </h3>
   <h4 id="Linear-transformation">
    Linear transformation
    <a class="anchor-link" href="#Linear-transformation">
     ¶
    </a>
   </h4>
   <p>
    Neural networks typically transform input vectors by matrix multiplication and vector addition followed by a non-linear transfer function. The 2-dimensional input vectors to our network $(x_{ik1}$, $x_{ik2})$ are transformed by a $2 \times 3$ weight matrix and a bias vector of size 3. Before they can be added to the states of the RNN. The 3-dimensional state vectors are transformed to a 1-dimensional output vector by a $3 \times 1$ weight matrix and a bias vector of size 1 to give the output probabilities.
   </p>
   <p>
    Since we want to process all inputs for each sample and each timestep in one computation we can use the numpy
    <a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.tensordot.html">
     <code>
      tensordot
     </code>
    </a>
    function to perform the dot products. This function takes 2 tensors and the axes that need to be aggregated by summation between the elements and a product of the result. For example the transformation of input X ($2000 \times 7 \times 2$) to the states S ($2000 \times 7 \times 3$) with the help of matrix W ($2 \times 3$) can be done by
    <code>
     S = tensordot(X, W, axes=((-1),(0)))
    </code>
    . This method will sum the elements of the last order (-1) of X with the elements of the first order (0) of W and multiply them together. This is the same as doing the matrix dot product for each $[x_{ik1}$, $x_{ik2}]$ vector with W.
    <code>
     tensordot
    </code>
    can then make sure that the underlying computations can be done efficiently and in parallel.
   </p>
   <p>
    These linear tensor transformations are used to transform the input X to the states S, and from the states S to the output Y. This linear transformation, together with its gradient is implemented in the
    <code>
     TensorLinear
    </code>
    class below. Note that the weights are initialized by sampling uniformly between $\pm \sqrt{6.0 / (n_{in} + n_{out})}$
    <a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">
     as suggested by X. Glorot
    </a>
    .
   </p>
   <h4 id="Logistic-classification">
    Logistic classification
    <a class="anchor-link" href="#Logistic-classification">
     ¶
    </a>
   </h4>
   <p>
    <a href="{% post_url 2015-06-10-cross_entropy_logistic %}">
     Logistic classification
    </a>
    is used to output the probability that the output at current time step k is 1. This function together with its loss and gradient is implemented in the
    <code>
     LogisticClassifier
    </code>
    class below.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [4]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython3">
     <pre><span></span><span class="c1"># Define the linear tensor transformation layer</span>
<span class="k">class</span> <span class="nc">TensorLinear</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""The linear tensor layer applies a linear tensor dot product </span>
<span class="sd">    and a bias to its input."""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">tensor_order</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">"""Initialse the weight W and bias b parameters."""</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_in</span> <span class="o">+</span> <span class="n">n_out</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">))</span> 
                  <span class="k">if</span> <span class="n">W</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">W</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_out</span><span class="p">))</span> <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">b</span><span class="p">)</span>
        <span class="c1"># Axes summed over in backprop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bpAxes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">tensor_order</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform forward step transformation with the help </span>
<span class="sd">        of a tensor product."""</span>
        <span class="c1"># Same as: Y[i,j,:] = np.dot(X[i,j,:], self.W) + self.b </span>
        <span class="c1">#          (for i,j in X.shape[0:1])</span>
        <span class="c1"># Same as: Y = np.einsum('ijk,kl-&gt;ijl', X, self.W) + self.b</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">),(</span><span class="mi">0</span><span class="p">)))</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">gY</span><span class="p">):</span>
        <span class="sd">"""Return the gradient of the parmeters and the inputs of </span>
<span class="sd">        this layer."""</span>
        <span class="c1"># Same as: gW = np.einsum('ijk,ijl-&gt;kl', X, gY)</span>
        <span class="c1"># Same as: gW += np.dot(X[:,j,:].T, gY[:,j,:]) </span>
        <span class="c1">#          (for i,j in X.shape[0:1])</span>
        <span class="n">gW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gY</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bpAxes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bpAxes</span><span class="p">))</span>
        <span class="n">gB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">gY</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bpAxes</span><span class="p">)</span>
        <span class="c1"># Same as: gX = np.einsum('ijk,kl-&gt;ijl', gY, self.W.T)</span>
        <span class="c1"># Same as: gX[i,j,:] = np.dot(gY[i,j,:], self.W.T) </span>
        <span class="c1">#          (for i,j in gY.shape[0:1])</span>
        <span class="n">gX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">gY</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">),(</span><span class="mi">0</span><span class="p">)))</span>  
        <span class="k">return</span> <span class="n">gX</span><span class="p">,</span> <span class="n">gW</span><span class="p">,</span> <span class="n">gB</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [5]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython3">
     <pre><span></span><span class="c1"># Define the logistic classifier layer</span>
<span class="k">class</span> <span class="nc">LogisticClassifier</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""The logistic layer applies the logistic function to its </span>
<span class="sd">    inputs."""</span>
   
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step transformation."""</span>
        <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Return the gradient with respect to the loss function </span>
<span class="sd">        at the inputs of this layer."""</span>
        <span class="c1"># Average by the number of samples and sequence length.</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Compute the loss at the output."""</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">T</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span> <span class="o">+</span> <span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">)))</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="Unfolding-the-recurrent-states">
    Unfolding the recurrent states
    <a class="anchor-link" href="#Unfolding-the-recurrent-states">
     ¶
    </a>
   </h3>
   <p>
    Just as in the
    <a href="{% post_url 2015-09-27-rnn_implementation_part01 %}">
     previous part
    </a>
    of this tutorial the recurrent states need to be unfolded through time. This unfolding during
    <a href="https://en.wikipedia.org/wiki/Backpropagation_through_time">
     backpropagation through time
    </a>
    is done by the
    <code>
     RecurrentStateUnfold
    </code>
    class. This class holds the shared weight and bias parameters used to update each state, as well as the initial state that is also treated as a parameter and optimized during backpropagation.
   </p>
   <p>
    The
    <code>
     forward
    </code>
    method of
    <code>
     RecurrentStateUnfold
    </code>
    iteratively updates the states through time and returns the resulting state tensor. The
    <code>
     backward
    </code>
    method propagates the gradients at the outputs of each state backwards through time. Note that at each time $k$ the gradient coming from the output Y needs to be added with the gradient coming from the previous state at time $k+1$. The gradients of the weight and bias parameters are summed over all timestep since they are shared parameters in each state update. The final state gradient at time $k=0$ is used to optimise the initial state $S_0$ since the gradient of the inital state is $\partial \xi / \partial S_{0}$.
   </p>
   <p>
    <code>
     RecurrentStateUnfold
    </code>
    makes use of the
    <code>
     RecurrentStateUpdate
    </code>
    class. The
    <code>
     forward
    </code>
    method of this class combines the transformed input and state at time $k-1$ to output state $k$. The
    <code>
     backward
    </code>
    method propagates the gradient backwards through time for one timestep and calculates the gradients of the parameters of this timestep. The non-linear transfer function used in
    <code>
     RecurrentStateUpdate
    </code>
    is the
    <a href="https://en.wikipedia.org/wiki/Hyperbolic_function">
     hyperbolic tangent
    </a>
    (tanh) function. This function, like the logistic function, is a
    <a href="https://en.wikipedia.org/wiki/Sigmoid_function">
     sigmoid function
    </a>
    that goes from $-1$ to $+1$. The
    <a href="https://theclevermachine.wordpress.com/tag/tanh-function/">
     tanh function
    </a>
    is chosen because the maximum gradient of this function is higher than the maximum gradient of the
    <a href="{% post_url 2015-06-10-neural_network_implementation_part02 %}#Logistic-function-and-cross-entropy-loss-function">
     logistic function
    </a>
    which make vanishing gradients less likely. This tanh transfer function is implemented in the
    <code>
     TanH
    </code>
    class.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [6]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython3">
     <pre><span></span><span class="c1"># Define tanh layer</span>
<span class="k">class</span> <span class="nc">TanH</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""TanH applies the tanh function to its inputs."""</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step transformation."""</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> 
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return the gradient at the inputs of this layer."""</span>
        <span class="n">gTanh</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="p">(</span><span class="n">Y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">gTanh</span> <span class="o">*</span> <span class="n">output_grad</span><span class="p">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [7]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython3">
     <pre><span></span><span class="c1"># Define internal state update layer</span>
<span class="k">class</span> <span class="nc">RecurrentStateUpdate</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""Update a given state."""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nbStates</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">"""Initialse the linear transformation and tanh transfer </span>
<span class="sd">        function."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">TensorLinear</span><span class="p">(</span><span class="n">nbStates</span><span class="p">,</span> <span class="n">nbStates</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span> <span class="o">=</span> <span class="n">TanH</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xk</span><span class="p">,</span> <span class="n">Sk</span><span class="p">):</span>
        <span class="sd">"""Return state k+1 from input and state k."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">Xk</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">Sk</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Sk0</span><span class="p">,</span> <span class="n">Sk1</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return the gradient of the parmeters and the inputs of </span>
<span class="sd">        this layer."""</span>
        <span class="n">gZ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">Sk1</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">)</span>
        <span class="n">gSk0</span><span class="p">,</span> <span class="n">gW</span><span class="p">,</span> <span class="n">gB</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">Sk0</span><span class="p">,</span> <span class="n">gZ</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gZ</span><span class="p">,</span> <span class="n">gSk0</span><span class="p">,</span> <span class="n">gW</span><span class="p">,</span> <span class="n">gB</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [8]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython3">
     <pre><span></span><span class="c1"># Define layer that unfolds the states over time</span>
<span class="k">class</span> <span class="nc">RecurrentStateUnfold</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""Unfold the recurrent states."""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nbStates</span><span class="p">,</span> <span class="n">nbTimesteps</span><span class="p">):</span>
        <span class="sd">"""Initialse the shared parameters, the inital state and </span>
<span class="sd">        state update function."""</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">6.</span> <span class="o">/</span> <span class="p">(</span><span class="n">nbStates</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">nbStates</span><span class="p">,</span> <span class="n">nbStates</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>  <span class="c1"># Shared bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">S0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nbStates</span><span class="p">)</span>  <span class="c1"># Initial state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nbTimesteps</span> <span class="o">=</span> <span class="n">nbTimesteps</span>  <span class="c1"># Timesteps to unfold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stateUpdate</span> <span class="o">=</span> <span class="n">RecurrentStateUpdate</span><span class="p">(</span>
            <span class="n">nbStates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>  <span class="c1"># State update function</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Iteratively apply forward step to all states."""</span>
        <span class="c1"># State tensor</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">S</span><span class="p">[:,</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">S0</span>  <span class="c1"># Set initial state</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nbTimesteps</span><span class="p">):</span>
            <span class="c1"># Update the states iteratively</span>
            <span class="n">S</span><span class="p">[:,</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stateUpdate</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="n">k</span><span class="p">,:],</span> <span class="n">S</span><span class="p">[:,</span><span class="n">k</span><span class="p">,:])</span>
        <span class="k">return</span> <span class="n">S</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">gY</span><span class="p">):</span>
        <span class="sd">"""Return the gradient of the parmeters and the inputs of </span>
<span class="sd">        this layer."""</span>
        <span class="c1"># Initialise gradient of state outputs</span>
        <span class="n">gSk</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">gY</span><span class="p">[:,</span><span class="bp">self</span><span class="o">.</span><span class="n">nbTimesteps</span><span class="o">-</span><span class="mi">1</span><span class="p">,:])</span>
        <span class="c1"># Initialse gradient tensor for state inputs</span>
        <span class="n">gZ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">gWSum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>  <span class="c1"># Initialise weight gradients</span>
        <span class="n">gBSum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>  <span class="c1"># Initialse bias gradients</span>
        <span class="c1"># Propagate the gradients iteratively</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nbTimesteps</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="c1"># Gradient at state output is gradient from previous state </span>
            <span class="c1">#  plus gradient from output</span>
            <span class="n">gSk</span> <span class="o">+=</span> <span class="n">gY</span><span class="p">[:,</span><span class="n">k</span><span class="p">,:]</span>
            <span class="c1"># Propgate the gradient back through one state</span>
            <span class="n">gZ</span><span class="p">[:,</span><span class="n">k</span><span class="p">,:],</span> <span class="n">gSk</span><span class="p">,</span> <span class="n">gW</span><span class="p">,</span> <span class="n">gB</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stateUpdate</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span>
                <span class="n">S</span><span class="p">[:,</span><span class="n">k</span><span class="p">,:],</span> <span class="n">S</span><span class="p">[:,</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">gSk</span><span class="p">)</span>
            <span class="n">gWSum</span> <span class="o">+=</span> <span class="n">gW</span>  <span class="c1"># Update total weight gradient</span>
            <span class="n">gBSum</span> <span class="o">+=</span> <span class="n">gB</span>  <span class="c1"># Update total bias gradient</span>
        <span class="c1"># Get gradient of initial state over all samples</span>
        <span class="n">gS0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">gSk</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gZ</span><span class="p">,</span> <span class="n">gWSum</span><span class="p">,</span> <span class="n">gBSum</span><span class="p">,</span> <span class="n">gS0</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="The-full-network">
    The full network
    <a class="anchor-link" href="#The-full-network">
     ¶
    </a>
   </h3>
   <p>
    The full network that will be trained to perform binary addition of two number is defined in the
    <code>
     RnnBinaryAdder
    </code>
    class below. It initialises all the layers upon creation. The
    <code>
     forward
    </code>
    method performs the full backpropagation forward step through all layers and timesteps and returns the intermediary outputs. The
    <code>
     backward
    </code>
    method performs the backward step through all layers and timesteps and returns the gradients of all the parameters. The
    <code>
     getParamGrads
    </code>
    method performs both steps and returns the gradients of the parameters in a list. The order of this list corresponds to the order of the iterator returned by
    <code>
     get_params_iter
    </code>
    . The parameters returned in the iterator of that last method are the same as the parameters of the network and can be used to change the parameters of the network manually.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [9]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython3">
     <pre><span></span><span class="c1"># Define the full network</span>
<span class="k">class</span> <span class="nc">RnnBinaryAdder</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""RNN to perform binary addition of 2 numbers."""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nb_of_inputs</span><span class="p">,</span> <span class="n">nb_of_outputs</span><span class="p">,</span> <span class="n">nb_of_states</span><span class="p">,</span> 
                 <span class="n">sequence_len</span><span class="p">):</span>
        <span class="sd">"""Initialse the network layers."""</span>
        <span class="c1"># Input layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensorInput</span> <span class="o">=</span> <span class="n">TensorLinear</span><span class="p">(</span><span class="n">nb_of_inputs</span><span class="p">,</span> <span class="n">nb_of_states</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="c1"># Recurrent layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span> <span class="o">=</span> <span class="n">RecurrentStateUnfold</span><span class="p">(</span><span class="n">nb_of_states</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">)</span>
        <span class="c1"># Linear output transform</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tensorOutput</span> <span class="o">=</span> <span class="n">TensorLinear</span><span class="p">(</span><span class="n">nb_of_states</span><span class="p">,</span> <span class="n">nb_of_outputs</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">LogisticClassifier</span><span class="p">()</span>  <span class="c1"># Classification output</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward propagation of input X through all </span>
<span class="sd">        layers."""</span>
        <span class="c1"># Linear input transformation</span>
        <span class="n">recIn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensorInput</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Forward propagate through time and return states</span>
        <span class="n">S</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">recIn</span><span class="p">)</span>
        <span class="c1"># Linear output transformation</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensorOutput</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">S</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="n">sequence_len</span><span class="o">+</span><span class="mi">1</span><span class="p">,:])</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>  <span class="c1"># Classification probabilities</span>
        <span class="c1"># Return: input to recurrent layer, states, input to classifier, </span>
        <span class="c1">#  output</span>
        <span class="k">return</span> <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Y</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Perform the backward propagation through all layers.</span>
<span class="sd">        Input: input samples, network output, intput to recurrent </span>
<span class="sd">        layer, states, targets."""</span>
        <span class="n">gZ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>  <span class="c1"># Get output gradient</span>
        <span class="n">gRecOut</span><span class="p">,</span> <span class="n">gWout</span><span class="p">,</span> <span class="n">gBout</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensorOutput</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span>
            <span class="n">S</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="n">sequence_len</span><span class="o">+</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">gZ</span><span class="p">)</span>
        <span class="c1"># Propagate gradient backwards through time</span>
        <span class="n">gRnnIn</span><span class="p">,</span> <span class="n">gWrec</span><span class="p">,</span> <span class="n">gBrec</span><span class="p">,</span> <span class="n">gS0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span>
            <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">gRecOut</span><span class="p">)</span>
        <span class="n">gX</span><span class="p">,</span> <span class="n">gWin</span><span class="p">,</span> <span class="n">gBin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tensorInput</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gRnnIn</span><span class="p">)</span>
        <span class="c1"># Return the parameter gradients of: linear output weights, </span>
        <span class="c1">#  linear output bias, recursive weights, recursive bias, #</span>
        <span class="c1">#  linear input weights, linear input bias, initial state.</span>
        <span class="k">return</span> <span class="n">gWout</span><span class="p">,</span> <span class="n">gBout</span><span class="p">,</span> <span class="n">gWrec</span><span class="p">,</span> <span class="n">gBrec</span><span class="p">,</span> <span class="n">gWin</span><span class="p">,</span> <span class="n">gBin</span><span class="p">,</span> <span class="n">gS0</span>
    
    <span class="k">def</span> <span class="nf">getOutput</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Get the output probabilities of input X."""</span>
        <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Y</span>
    
    <span class="k">def</span> <span class="nf">getBinaryOutput</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Get the binary output of input X."""</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">getParamGrads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Return the gradients with respect to input X and </span>
<span class="sd">        target T as a list. The list has the same order as the </span>
<span class="sd">        get_params_iter iterator."""</span>
        <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">gWout</span><span class="p">,</span> <span class="n">gBout</span><span class="p">,</span> <span class="n">gWrec</span><span class="p">,</span> <span class="n">gBrec</span><span class="p">,</span> <span class="n">gWin</span><span class="p">,</span> <span class="n">gBin</span><span class="p">,</span> <span class="n">gS0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">recIn</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">g</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gS0</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gWin</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gBin</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gWrec</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gBrec</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gWout</span><span class="p">),</span>
                <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">gBout</span><span class="p">))]</span>
    
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Return the loss of input X w.r.t. targets T."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get_params_iter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Return an iterator over the parameters.</span>
<span class="sd">        The iterator has the same order as get_params_grad.</span>
<span class="sd">        The elements returned by the iterator are editable in-place."""</span>
        <span class="k">return</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span><span class="o">.</span><span class="n">S0</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensorInput</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensorInput</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rnnUnfold</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensorOutput</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]),</span> 
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensorOutput</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]))</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Gradient-Checking">
    Gradient Checking
    <a class="anchor-link" href="#Gradient-Checking">
     ¶
    </a>
   </h2>
   <p>
    As in
    <a href="{% post_url 2015-06-10-neural_network_implementation_part04 %}#Gradient-checking">
     part 4 of our previous tutorial on feedforward nets
    </a>
    the gradient computed by backpropagation is compared with the
    <a href="https://en.wikipedia.org/wiki/Numerical_differentiation">
     numerical gradient
    </a>
    to assert that there are no bugs in the code to compute the gradients. This is done by the code below.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [10]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython3">
     <pre><span></span><span class="c1"># Do gradient checking</span>
<span class="c1"># Define an RNN to test</span>
<span class="n">RNN</span> <span class="o">=</span> <span class="n">RnnBinaryAdder</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">)</span>
<span class="c1"># Get the gradients of the parameters from a subset of the data</span>
<span class="n">backprop_grads</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">getParamGrads</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:],</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:])</span>

<span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-7</span>  <span class="c1"># Set the small change to compute the numerical gradient</span>
<span class="c1"># Compute the numerical gradients of the parameters in all layers.</span>
<span class="k">for</span> <span class="n">p_idx</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">()):</span>
    <span class="n">grad_backprop</span> <span class="o">=</span> <span class="n">backprop_grads</span><span class="p">[</span><span class="n">p_idx</span><span class="p">]</span>
    <span class="c1"># + eps</span>
    <span class="n">param</span> <span class="o">+=</span> <span class="n">eps</span>
    <span class="n">plus_loss</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span>
        <span class="n">RNN</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:]),</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:])</span>
    <span class="c1"># - eps</span>
    <span class="n">param</span> <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">eps</span>
    <span class="n">min_loss</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span>
        <span class="n">RNN</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:]),</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:])</span>
    <span class="c1"># reset param value</span>
    <span class="n">param</span> <span class="o">+=</span> <span class="n">eps</span>
    <span class="c1"># calculate numerical gradient</span>
    <span class="n">grad_num</span> <span class="o">=</span> <span class="p">(</span><span class="n">plus_loss</span> <span class="o">-</span> <span class="n">min_loss</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">eps</span><span class="p">)</span>
    <span class="c1"># Raise error if the numerical grade is not close to the </span>
    <span class="c1">#  backprop gradient</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">grad_num</span><span class="p">,</span> <span class="n">grad_backprop</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span>
            <span class="n">f</span><span class="s1">'Numerical gradient of </span><span class="si">{grad_num:.6f}</span><span class="s1"> is not close '</span>
            <span class="n">f</span><span class="s1">'to the backpropagation gradient of </span><span class="si">{grad_backprop:.6f}</span><span class="s1">!'</span>
        <span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'No gradient errors found'</span><span class="p">)</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>No gradient errors found
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Rmsprop-with-momentum-optimisation">
    Rmsprop with momentum optimisation
    <a class="anchor-link" href="#Rmsprop-with-momentum-optimisation">
     ¶
    </a>
   </h2>
   <p>
    While the
    <a href="{% post_url 2015-09-27-rnn_implementation_part01 %}">
     first part
    </a>
    of this tutorial used
    <a href="https://en.wikipedia.org/wiki/Rprop">
     Rprop
    </a>
    to optimise the network, this part will use the
    <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">
     Rmsprop
    </a>
    algorithm with
    <a href="http://www.cs.toronto.edu/~fritz/absps/momentum.pdf">
     Nesterov's accelerated gradient
    </a>
    to perform the optimisation. We replaced the Rprop algorithm because Rprop doesn't work well with minibatches due to the stochastic nature of the error surface that can result in sign changes of the gradient.
   </p>
   <p>
    The Rmsprop algorithm was inspired by the Rprop algorithm. It keeps a
    <a href="https://en.wikipedia.org/wiki/Moving_average">
     moving average
    </a>
    (MA) of the squared gradient for each parameter $\theta$ ($MA = \lambda \cdot MA + (1-\lambda) \cdot (\partial \xi / \partial \theta)^2$, with $\lambda$ the moving average hyperparameter). The gradient is then normalised by dividing by the square root of this moving average (
    <code>
     maSquare
    </code>
    = $(\partial \xi / \partial \theta)/\sqrt{MA}$). This normalised gradient is then used to update the parameters. Note that if $\lambda=0$ the gradient is reduced to its sign.
   </p>
   <p>
    This transformed gradient is not used directly to update the parameters, but it is used to update a momentum parameter (
    <code>
     Vs
    </code>
    ) for each parameter of the network. This parameter is similar to the momentum parameter from our
    <a href="{% post_url 2015-06-10-neural_network_implementation_part04 %}#Backpropagation-updates-with-momentum">
     previous tutorial
    </a>
    but it is used in a slightly different way. Nesterov's accelerated gradient is different from regular momentum in that it applies updates in a different way. While the regular momentum algorithm calculates the gradient at the beginning of the iteration, updates the momentum and moves the parameters according to this momentum, Nesterov's accelerated gradient moves the parameters according to the reduced momentum, then calculates the gradients, updates the momentum, and then moves again according to the local gradient. This has as benefit that the gradient is more informative to do the local update, and can correct for a bad momentum update. The Nesterov updates can be described as:
   </p>
   <p>
    $$
\begin{split}
V_{i+1} &amp; = \lambda V_i - \mu \nabla(\theta_i + \lambda V_i) \\
\theta_{i+1} &amp; = \theta_i + V_{i+1} \\
\end{split}
$$
   </p>
   <p>
    With $\nabla(\theta)$ the local gradient at position $\theta$ in the parameter space. And $i$ the iteration number. This formula can be visualised as in the following illustration (See
    <a href="http://www.cs.toronto.edu/~fritz/absps/momentum.pdf">
     Sutskever I.
    </a>
    ):
   </p>
   <p>
    <img alt="Illustration of Nesterov Momentum updates" src="/images/RNN_implementation/NesterovMomentum.png"/>
   </p>
   <p>
    Note that the training converges to a loss of 0. This convergence is actually not guaranteed. If the parameters of the network start out in a bad position the network might convert to a local minimum that is far from the global minimum. The training is also sensitive to the meta parameters
    <code>
     lmbd
    </code>
    ,
    <code>
     learning_rate
    </code>
    ,
    <code>
     momentum_term
    </code>
    ,
    <code>
     eps
    </code>
    . Try rerunning this yourself to see how many times it actually converges.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [11]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython3">
     <pre><span></span><span class="c1"># Set hyper-parameters</span>
<span class="n">lmbd</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># Rmsprop lambda</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.05</span>  <span class="c1"># Learning rate</span>
<span class="n">momentum_term</span> <span class="o">=</span> <span class="mf">0.80</span>  <span class="c1"># Momentum term</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span>  <span class="c1"># Numerical stability term to prevent division by zero</span>
<span class="n">mb_size</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Size of the minibatches (number of samples)</span>

<span class="c1"># Create the network</span>
<span class="n">nb_of_states</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Number of states in the recurrent layer</span>
<span class="n">RNN</span> <span class="o">=</span> <span class="n">RnnBinaryAdder</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nb_of_states</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">)</span>
<span class="c1"># Set the initial parameters</span>
<span class="c1"># Number of parameters in the network</span>
<span class="n">nbParameters</span> <span class="o">=</span>  <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">RNN</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">())</span>
<span class="c1"># Rmsprop moving average</span>
<span class="n">maSquare</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nbParameters</span><span class="p">)]</span>
<span class="n">Vs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nbParameters</span><span class="p">)]</span>  <span class="c1"># Momentum</span>

<span class="c1"># Create a list of minibatch losses to be plotted</span>
<span class="n">ls_of_loss</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">RNN</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:]),</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">100</span><span class="p">,:,:])]</span>
<span class="c1"># Iterate over some iterations</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="c1"># Iterate over all the minibatches</span>
    <span class="k">for</span> <span class="n">mb</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_train</span> <span class="o">//</span> <span class="n">mb_size</span><span class="p">):</span>
        <span class="n">X_mb</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">mb</span><span class="p">:</span><span class="n">mb</span><span class="o">+</span><span class="n">mb_size</span><span class="p">,:,:]</span>  <span class="c1"># Input minibatch</span>
        <span class="n">T_mb</span> <span class="o">=</span> <span class="n">T_train</span><span class="p">[</span><span class="n">mb</span><span class="p">:</span><span class="n">mb</span><span class="o">+</span><span class="n">mb_size</span><span class="p">,:,:]</span>  <span class="c1"># Target minibatch</span>
        <span class="n">V_tmp</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span> <span class="o">*</span> <span class="n">momentum_term</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">Vs</span><span class="p">]</span>
        <span class="c1"># Update each parameters according to previous gradient</span>
        <span class="k">for</span> <span class="n">pIdx</span><span class="p">,</span> <span class="n">P</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">()):</span>
            <span class="n">P</span> <span class="o">+=</span> <span class="n">V_tmp</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span>
        <span class="c1"># Get gradients after following old velocity</span>
        <span class="c1"># Get the parameter gradients</span>
        <span class="n">backprop_grads</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">getParamGrads</span><span class="p">(</span><span class="n">X_mb</span><span class="p">,</span> <span class="n">T_mb</span><span class="p">)</span>    
        <span class="c1"># Update each parameter seperately</span>
        <span class="k">for</span> <span class="n">pIdx</span><span class="p">,</span> <span class="n">P</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">()):</span>
            <span class="c1"># Update the Rmsprop moving averages</span>
            <span class="n">maSquare</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">=</span> <span class="n">lmbd</span> <span class="o">*</span> <span class="n">maSquare</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span>
                <span class="mi">1</span><span class="o">-</span><span class="n">lmbd</span><span class="p">)</span> <span class="o">*</span> <span class="n">backprop_grads</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
            <span class="c1"># Calculate the Rmsprop normalised gradient</span>
            <span class="n">pGradNorm</span> <span class="o">=</span> <span class="p">((</span>
                <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">backprop_grads</span><span class="p">[</span><span class="n">pIdx</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
                <span class="n">maSquare</span><span class="p">[</span><span class="n">pIdx</span><span class="p">])</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
            <span class="c1"># Update the momentum</span>
            <span class="n">Vs</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">=</span> <span class="n">V_tmp</span><span class="p">[</span><span class="n">pIdx</span><span class="p">]</span> <span class="o">-</span> <span class="n">pGradNorm</span>     
            <span class="n">P</span> <span class="o">-=</span> <span class="n">pGradNorm</span>   <span class="c1"># Update the parameter</span>
        <span class="c1"># Add loss to list to plot</span>
        <span class="n">ls_of_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">RNN</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">X_mb</span><span class="p">),</span> <span class="n">T_mb</span><span class="p">))</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [12]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython3">
     <pre><span></span><span class="c1"># Plot the loss over the iterations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ls_of_loss</span><span class="p">,</span> <span class="s1">'b-'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'minibatch iteration'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'$</span><span class="se">\\</span><span class="s1">xi$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Decrease of loss over backprop iteration'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_png output_subarea ">
     <img src="data:image/png;base64,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
"/>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Test-examples">
    Test examples
    <a class="anchor-link" href="#Test-examples">
     ¶
    </a>
   </h2>
   <p>
    The figure above shows that the training converged to a loss of 0. We expect the network to have learned how to perfectly do binary addition for our training examples. If we put some independent test cases through the network and print them out we can see that the network also outputs the correct output for these test cases.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [13]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class=" highlight hl-ipython3">
     <pre><span></span><span class="c1"># Create test samples</span>
<span class="n">nb_test</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">Xtest</span><span class="p">,</span> <span class="n">Ttest</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">nb_test</span><span class="p">,</span> <span class="n">sequence_len</span><span class="p">)</span>
<span class="c1"># Push test data through network</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">getBinaryOutput</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
<span class="n">Yf</span> <span class="o">=</span> <span class="n">RNN</span><span class="o">.</span><span class="n">getOutput</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>

<span class="c1"># Print out all test examples</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Xtest</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">printSample</span><span class="p">(</span><span class="n">Xtest</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">Xtest</span><span class="p">[</span><span class="n">i</span><span class="p">,:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">Ttest</span><span class="p">[</span><span class="n">i</span><span class="p">,:,:],</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,:,:])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>x1:   0100010   34
x2: + 1100100   19
      -------   --
t:  = 1010110   53
y:  = 1010110

x1:   1010100   21
x2: + 1110100   23
      -------   --
t:  = 0011010   44
y:  = 0011010

x1:   1111010   47
x2: + 0000000    0
      -------   --
t:  = 1111010   47
y:  = 1111010

x1:   1000000    1
x2: + 1111110   63
      -------   --
t:  = 0000001   64
y:  = 0000001

x1:   1010100   21
x2: + 1010100   21
      -------   --
t:  = 0101010   42
y:  = 0101010

</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <p>
    This post at
    <a href="http://peterroelants.github.io/">
     peterroelants.github.io
    </a>
    is generated from an IPython notebook file.
    <a href="https://github.com/peterroelants/peterroelants.github.io/blob/master/notebooks/RNN_implementation/rnn_implementation_part02.ipynb">
     Link to the full IPython notebook file
    </a>
   </p>
  </div>
 </div>
</div>
